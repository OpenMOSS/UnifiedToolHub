# torch # 应该优先根据服务器的显卡、CUDA版本、训练需求和其他依赖来安装对应的 torch 版本
vllm # 需要根据服务器的显卡、CUDA版本、torch 版本等来安装对应的 vllm 版本
transformers # 根据 torch 和 vllm 的版本来安装对应的 transformers 版本
